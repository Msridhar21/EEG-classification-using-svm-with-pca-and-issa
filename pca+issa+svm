# load a BCICIV_calib_ds1d.mat file
import scipy.io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import mlab
import seaborn as sns 

# detail information about the data file
m = scipy.io.loadmat(r'BCICIV_calib_ds1d.mat',struct_as_record = True)
print(m)
sample_rate = m['nfo']['fs'][0][0][0][0]
EEG = m['cnt'].T
nchannels, nsamples = EEG.shape

channel_names = [s[0] for s in m['nfo']['clab'][0][0][0]]
event_onsets = m['mrk'][0][0][0]
event_codes = m['mrk'][0][0][1]

labels = np.zeros((1,nsamples),int)
labels[0, event_onsets] = event_codes

cl_lab = [s[0] for s in m['nfo']['classes'][0][0][0]]
cl1 = cl_lab[0]
cl2 = cl_lab[1]

nclasses = len(cl_lab)
nevents = len(event_onsets)

print('shape of EEG: ',EEG.shape)
print('sample rate: ',sample_rate)
print('Number of channels: ',nchannels)
print('Channel names: ',channel_names)
print('Number of Events: ',len(event_onsets))
print('Event codes: ',np.unique(event_codes))
print('Class labels: ',cl_lab)
print('Number of classes: ',nclasses)

# crating trials
trials = {}

win = np.arange(int(0.5*sample_rate),int(2.5*sample_rate))
nsamples = len(win)

for cl, code in zip(cl_lab, np.unique(event_codes)):

    cl_onsets = event_onsets[event_codes == code]

    trials[cl] = np.zeros((nchannels, nsamples, len(cl_onsets)))

    for i,onset in enumerate(cl_onsets):
        trials[cl][:,:,i] = EEG[:, win+int(onset*0.1)]

print('Shape of trials[cl1]: ',trials[cl1].shape)
print(trials[cl1])
print('Shape of trials[cl2]: ',trials[cl2].shape)
print(trials[cl2])

# creating psd values
def psd(trials):
    ntrials = trials.shape[2]
    trials_PSD = np.zeros((nchannels, 101, ntrials))

    for trial in range(ntrials):
        for ch in range(nchannels):

            (PSD,freqs) = mlab.psd(trials[ch,:,trial],NFFT=int(nsamples),Fs=sample_rate)
            trials_PSD[ch,:,trial] = PSD.ravel()

    return trials_PSD,freqs


psd_r, freqs = psd(trials[cl1])
psd_f, freqs = psd(trials[cl2])
trials_PSD = {cl1: psd_r, cl2: psd_f}
print(trials_PSD)

# plotting psd values of left and right
def plot_psd(trials_PSD, freqs, chan_ind, chan_lab=None, maxy = None):

    plt.figure(figsize = (12,12))
    nchans = len(chan_ind)

    ncols = np.ceil(nchans/3)
    nrows = min(3,nchans)

    for i,ch in enumerate(chan_ind):
        plt.subplot(int(nrows),int(ncols),i+1)

        for cl in trials.keys():
            plt.plot(freqs, np.mean(trials_PSD[cl][ch,:,:],axis = 1), label = cl)

        plt.xlim(1,30)

        if(maxy != None):
            plt.ylim(0,maxy)
        else:
            plt.ylim(1,1200)

        plt.grid()

        plt.xlabel('Frequency (Hz)')
        plt.ylabel('Power (Wt)')


        plt.title(chan_lab[i])

        plt.xticks([4,8,12,15,18,30,50], [4,8,12,15,18,30,50])

        plt.legend()

    plt.tight_layout()

plot_psd(trials_PSD,freqs, [channel_names.index(ch) for ch in ['C3','C4']], chan_lab = ['left','right'],maxy = 500)

# plotting different frequencies of 'delta', 'theta', 'alpha', 'low-beta','mid-beta','high-beta', 'gamma' signals in channels 'C3', 'Cz', 'C4'
def plot_diff_freqs(trials_PSD, useful_chan, max_x = None, max_y = None):
    #useful_chan = ['C3', 'Cz', 'C4']

    fig = plt.figure(figsize = (12,12))
    nchans = len(useful_chan)
    ncols = np.ceil(nchans/3)
    nrows = min(3,nchans)


    for i in range(len(useful_chan)):
        plt.subplot(int(nrows),int(ncols),i+1)

        chan = useful_chan[i]
        plt.plot(freqs, abs(np.mean(trials_PSD['left'][channel_names.index(chan),:,:],axis = 1) - np.mean(trials_PSD['right'][channel_names.index(chan),:,:],axis = 1)))
        if(max_x):
            plt.xlim(1,max_x)
        else:
            plt.xlim(1,30)
        if(max_y):
            plt.ylim(1,max_y)
        else:
            plt.ylim(0,1200)
        plt.grid()
        plt.title(str(useful_chan[i]))

        plt.xticks([4,8,12,15,18,30,50], ['delta', 'theta', 'alpha', 'low-beta','mid-beta','high-beta', 'gamma'])

    plt.tight_layout()
    plt.xlabel("Difference between psd values of the different classes")

plot_diff_freqs(trials_PSD,['C3', 'Cz', 'C4'])
# https://nhahealth.com/brainwaves-the-language/#:~:text=The%20raw%20EEG%20has%20usually,(less%20than%204%20Hz).
#From various resources from the net, we conclude that mostly alpha and low beta waves (8-15 hz) contribute to the mind/body coordination, integration & associated tasks

#plotting psd of channels c3,c4,cz
from scipy import signal
def bpf(trials, bpf_range, N):
    a, b = scipy.signal.iirfilter(6, [float(float(x)/50.0) for x in  bpf_range])

    trial_keys = list(trials.keys())
    ntrials = trials[trial_keys[0]].shape[2]
    trials_bpf = {}
    for j in trial_keys:
        trials_bpf[j] = np.zeros((nchannels, nsamples, ntrials))
        for i in range(ntrials):
            trials_bpf[j][:,:,i] = signal.lfilter(a, b, trials[j][:,:,i], axis=1)

    return trials_bpf

trials_bpf = bpf(trials, [8,15], 6)

psd_r, freqs = psd(trials_bpf[cl1])
psd_f, freqs = psd(trials_bpf[cl2])

trials_psd_bpf = {cl1:psd_r, cl2:psd_f}



plot_psd(trials_psd_bpf,freqs, [channel_names.index(ch) for ch in ['C3','Cz','C4']], chan_lab = ['C3','Cz','C4'],maxy = 200)

plot_diff_freqs(trials_psd_bpf, ['C3', 'Cz', 'C4'], max_y = 50)

#calcualting log variance
trails_logvar_cl1 = np.log(np.var(trials_psd_bpf[cl1], axis = 1))
trails_logvar_cl2 = np.log(np.var(trials_psd_bpf[cl2], axis = 1))
trails_logvar_feat = {cl1: trails_logvar_cl1, cl2: trails_logvar_cl2}
def plot_logvar(trails):
    plt.figure(figsize = (12,6))

    x_data = list(range(nchannels))

    y1 = np.mean(trails[cl1], axis = 1)
    y2 = np.mean(trails[cl2], axis = 1)

    plt.plot(x_data, y1)
    plt.plot(x_data, y2)

    plt.legend(['left', 'right'])

    plt.xlabel('channels')
    plt.ylabel('log variances values')
    plt.title('log variances of each channel')

plot_logvar(trails_logvar_feat)

# calculating csp 
def plot_logvar(trails):
    plt.figure(figsize = (12,6))

    x_data = list(range(nchannels))

    y1 = np.mean(trails[cl1], axis = 1)
    y2 = np.mean(trails[cl2], axis = 1)

    plt.plot(x_data, y1)
    plt.plot(x_data, y2)

    plt.legend(['left', 'right'])

    plt.xlabel('channels')
    plt.ylabel('log variances values')
    plt.title('log variances of each channel')

plot_logvar(trails_logvar_feat)

W = csp(trials_bpf[cl1], trials_bpf[cl2])
trials_bpf_csp = {cl1: apply_mix(W, trials_bpf[cl1]),
              cl2: apply_mix(W, trials_bpf[cl2])}

trials_logvar_csp = {cl1: np.log(np.var(trials_bpf_csp[cl1],axis = 1)),
                 cl2: np.log(np.var(trials_bpf_csp[cl2], axis = 1))}
plot_logvar(trials_logvar_csp)

psd_r, freqs = psd(trials_bpf_csp[cl1])
psd_f, freqs = psd(trials_bpf_csp[cl2])

trials_psd_bpf_csp = {cl1:psd_r, cl2:psd_f}

plot_psd(trials_psd_bpf_csp, freqs, [0,58,-1], chan_lab=['C3','Cz','C4'], maxy = 0.2)


#applying csp on both training and testing data, creating a data frame from them
split_percentage = 0.75

ntrain_r = int(trials_bpf[cl1].shape[2] * split_percentage)
ntrain_f = int(trials_bpf[cl2].shape[2] * split_percentage)

train = {cl1: trials_bpf[cl1][:,:,:ntrain_r],
         cl2: trials_bpf[cl2][:,:,:ntrain_f]}

test = {cl1: trials_bpf[cl1][:,:,ntrain_r:],
        cl2: trials_bpf[cl2][:,:,ntrain_f:]}


W = csp(train[cl1], train[cl2])

# Apply the CSP on both the training and test set
train[cl1] = apply_mix(W, train[cl1])
train[cl2] = apply_mix(W, train[cl2])
test[cl1] = apply_mix(W, test[cl1])
test[cl2] = apply_mix(W, test[cl2])

comp = np.array([0,-1,30,40,20])
comp = np.array([channel_names.index(ch) for ch in ['C3','Cz','C4']])
comp = np.array(list(range(59)))
train[cl1] = train[cl1][comp,:,:]
train[cl2] = train[cl2][comp,:,:]
test[cl1] = test[cl1][comp,:,:]
test[cl2] = test[cl2][comp,:,:]


train[cl1] = np.log(np.var((train[cl1]), axis = 1))
train[cl2] = np.log(np.var((train[cl2]),axis = 1))
test[cl1] = np.log(np.var((test[cl2]),axis = 1))
test[cl2] = np.log(np.var((test[cl2]),axis = 1))

#df1 = pd.DataFrame()
df2 = pd.DataFrame()
Y1 = []
Y2 = []

for i in range(len(comp)):
    df1[channel_names[comp[i]]] = np.append(train[cl1][i],test[cl1][i])
    df2[channel_names[comp[i]]] = np.append(train[cl2][i],test[cl2][i])


Y1 = [0]*len(df1)
Y2 = [1]*len(df2)
Y_final = Y1 + Y2

df_final = pd.concat([df1, df2])
df_total = df_final.copy()
df_total['Y'] = Y_final

#print(Y_final)
#print("################################################################################################################")
print(df_final)
#print("################################################################################################################")
df_total = df_total.sample(frac = 1)
print(df_total)

# confusion_matrix
def my_confusion_matrix(predicted, Y_test):
    l = len(predicted)
    check = [predicted[i] == Y_test[i] for i in range(l)]

    conf = np.array([
   [ [predicted[i]==Y_test[i]==1 for i in range(l)].count(1), Y_test.count(1) - [predicted[i]==Y_test[i]==1 for i in range(l)].count(1) ],
    [ Y_test.count(0) -[predicted[i]==Y_test[i]==0 for i in range(l)].count(1), [predicted[i]==Y_test[i]==0 for i in range(l)].count(1) ],
])
    print("The Confusion Matrix is given as: ")
    print(conf)
    acc = [predicted[i] == Y_test[i] for i in range(len(predicted))].count(1) / len(predicted)
    print("the accuracy is: ",acc)

#split the data into training and test sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_final, Y_final, test_size=0.25, random_state=400)

#applying standard scalar before going to svm
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#using PCA as pre processor to reduce the dimensions in the training and testing sets 
from sklearn.decomposition import PCA
# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)  # Assuming you want to reduce to 2 dimensions
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)




#using improved ssa tuning the parameters of svm 
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC

class ImprovedSSA:
    def __init__(self, n_iterations, n_squirrels, alpha=0.1):
        self.n_iterations = n_iterations
        self.n_squirrels = n_squirrels
        self.alpha = alpha

    def optimize(self, X_train_pca, y_train, X_test_pca, y_test,kernel):
        best_score = -np.inf
        best_params = None

        for _ in range(self.n_iterations):
            for _ in range(self.n_squirrels):
                
                params = {
                    'C': np.random.uniform(0.1,1),
                    'gamma': np.random.uniform(0.01, 0.1),
                    'kernel': kernel 
                }

                
                svm = SVC(**params)
                svm.fit(X_train_pca, y_train)

               
                y_pred = svm.predict(X_test_pca)
                score = accuracy_score(y_test, y_pred)

                if score > best_score:
                    best_score = score
                    best_params = params

            for param in best_params:
                if param != 'kernel':  # Skip concatenation for the kernel parameter
                   # best_params[param] += self.alpha * np.random.uniform(-1, 1)
                     best_params[param] += self.alpha * np.random.uniform(-1, 1)
                else:
                    best_params[param] = kernel  # Ensure kernel remains the same
        global y
        y=y_pred
        return best_params, best_score

ssa = ImprovedSSA(n_iterations=100, n_squirrels=5)
best_params, best_score = ssa.optimize(X_train_pca, y_train, X_test_pca, y_test,kernel='rbf')

print("Best parameters found:", best_params)
print("Best validation accuracy:", best_score)

my_confusion_matrix(y, y_test)

#training and testing the svm model
SVMclassifier = SVC(kernel = 'rbf', C=0.030,gamma= 0.350).fit(X_train_pca, y_train)
rbfPredict = SVMclassifier.predict(X_test_pca)
print("Output : ")
my_confusion_matrix(rbfPredict, y_test)

#plotting the confusion matrix
cm=confusion_matrix(rbfPredict, y_test)
plt.figure(figsize=(8, 6))
sns.set(font_scale=1.2) 
sns.heatmap(cm, annot=True, fmt='d',cmap="Blues", annot_kws={"size": 14})
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.title("Confusion Matrix")
plt.show()


print('')
print(*rbfPredict)
print(*y_test)

scores = cross_val_score(rbfSVMclassifier, X_test_pca, y_test, cv=10)
print(scores)
